# **BMEN 415 - Final Project**
**Classification Model 1: Linear Discriminant Analysis**

Yara Elhariry
### **Data Preprocessing, Model Creation & Training the Model**
#### Importing Libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score
import plotly.graph_objects as go
#### Importing Training Dataset
df_train = pd.read_csv('Classification - Training.csv')
df_train = df_train.drop(df_train.columns[0:2], axis = 1)
df_train
#### Checking for any Missing Values in the Dataset
df_train.isnull().sum()
#### Correlation Heat Map
%matplotlib inline
import seaborn as sns
import matplotlib.pyplot as plt

correlations = df_train.corr()
sns.heatmap(correlations)
plt.show()
#### Splitting Training Dataset into Independent and Dependent Variable (X and y Datasets)
X = df_train.drop(columns = ['diagnosis'])
X
y = pd.DataFrame(df_train['diagnosis'], columns = ['diagnosis'])
y
#### Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X)
print(X_train)
#### Creating the Model & Training the LDA Model on the Training Set
lda = LDA()
lda.fit(X_train, y.values.ravel())
### **Implementing Testing Data to the Model Created**
#### Importing the Testing Data
df_test = pd.read_csv('Classification - Testing.csv')
df_test = df_test.drop(df_test.columns[0:2], axis = 1)
df_test
#### Checking for any Missing Values in the Dataset
df_test.isnull().sum()
#### Splitting Testing Dataset into Independent and Dependent Variable (X and y Datasets)
X_test = df_test.drop(columns = ['diagnosis'])
X_test
y_test = pd.DataFrame(df_test['diagnosis'], columns = ['diagnosis'])
y_test
#### Feature Scaling
X_test = sc.transform(X_test)
#### Predicting
y_pred = lda.predict(X_test)
y_pred
#### Comparing Predictions with the Actual
y_pred = pd.DataFrame(y_pred, columns=['y Predicted'])
df_predictions = pd.concat([y_test, y_pred], axis=1)
df_predictions.columns = ['y Actual', 'y Predicted']
df_predictions
#### Counting Malignant and Benign Occurence in the Actual and Predicted Datasets
df_counts_a = df_predictions['y Actual'].value_counts().reset_index()
df_counts_p = df_predictions['y Predicted'].value_counts().reset_index()
df_counts = pd.concat([df_counts_a, df_counts_p], axis=1)
df_counts.columns = ['y Actual Value', 'y Actual Count', 'y Predicted Value', 'y Predicted Count']
df_counts

#### Visual Comparisons
import plotly.graph_objects as go

samples = ['Benign', 'Malignant']

fig = go.Figure()

fig.add_trace(go.Bar(
    x=samples,
    y=df_counts['y Actual Count'],
    name='y Actual',
    marker_color='indianred'
))

fig.add_trace(go.Bar(
    x=samples,
    y=df_counts['y Predicted Count'],
    name='y Predicted',
    marker_color='lightsalmon'
))

fig.update_layout(
    title="y Actual vs y Predicted",
    yaxis_title='Count',
    barmode='group',
    xaxis_tickangle=-45
)

fig.show()
#### Confusion Matrix & Metrics
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score
import seaborn as sns

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True)

print('F1 Score:', f1_score(y_test, y_pred, average=None))
print('Precision:', precision_score(y_test, y_pred, average=None))
print('Recall:', recall_score(y_test, y_pred, average=None))
print('Accuracy:', accuracy_score(y_test, y_pred))
